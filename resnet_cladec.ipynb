{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3h_24XnYdlzz"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f66073b394b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-envs/ml_def/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2346\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2348\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-envs/ml_def/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     \u001b[0mevaldict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_func_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_e%d_'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fix' is not defined"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tEg2glG-CnB",
    "outputId": "a7876b8d-0e24-4183-92d5-7ae1d11a0324"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "qXPxQrYege-D"
   },
   "outputs": [],
   "source": [
    "import torchvision, torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import numpy as np, sklearn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from shutil import copyfile\n",
    "from resnet import ResNet10\n",
    "from torchvision.transforms.transforms import CenterCrop, Resize\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "gds = lambda dataset, cfg: torch.utils.data.DataLoader(\n",
    "    TensorDataset(*[torch.from_numpy(x) for x in dataset]), batch_size=cfg[\"batchSize\"]\n",
    ")\n",
    "\n",
    "\n",
    "def getnorm(dname):\n",
    "    if dname == \"Ci10\":\n",
    "        return (\n",
    "            torch.from_numpy(\n",
    "                np.array((0.4914, 0.4822, 0.4465), np.float32).reshape(1, 3, 1, 1)\n",
    "            ).cuda(),\n",
    "            torch.from_numpy(\n",
    "                np.array((0.2023, 0.1994, 0.2010), np.float32).reshape(1, 3, 1, 1)\n",
    "            ).cuda(),\n",
    "        )\n",
    "    elif dname == \"Ci100\":\n",
    "        return (\n",
    "            torch.from_numpy(\n",
    "                np.array((0.5060725, 0.48667726, 0.4421305), np.float32).reshape(\n",
    "                    1, 3, 1, 1\n",
    "                )\n",
    "            ).cuda(),\n",
    "            torch.from_numpy(\n",
    "                np.array((0.2675421, 0.25593522, 0.27593908), np.float32).reshape(\n",
    "                    1, 3, 1, 1\n",
    "                )\n",
    "            ).cuda(),\n",
    "        )\n",
    "    elif dname == \"Fash\":\n",
    "        return (\n",
    "            torch.from_numpy(np.array((0.281), np.float32).reshape(1, 1, 1, 1)).cuda(),\n",
    "            torch.from_numpy(np.array((0.352), np.float32).reshape(1, 1, 1, 1)).cuda(),\n",
    "        )\n",
    "    elif dname == \"MNIST\":\n",
    "        return (\n",
    "            torch.from_numpy(np.array((0.1307), np.float32).reshape(1, 1, 1, 1)).cuda(),\n",
    "            torch.from_numpy(np.array((0.3081), np.float32).reshape(1, 1, 1, 1)).cuda(),\n",
    "        )\n",
    "    elif dname == \"TinyImgNet\":\n",
    "        return (\n",
    "            torch.from_numpy(\n",
    "                np.array((0.4802, 0.4481, 0.3975), np.float32).reshape(3, 1, 1)\n",
    "            ),\n",
    "            torch.from_numpy(\n",
    "                np.array((0.2302, 0.2265, 0.2262), np.float32).reshape(3, 1, 1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "def getFullDS(cfg, reconstr=False):\n",
    "    dname = cfg[\"ds\"][0]\n",
    "    trans = transforms.Compose([transforms.ToTensor()])\n",
    "    refu = lambda x: F.interpolate(x.unsqueeze(0), size=64).squeeze(0)\n",
    "    if dname == \"Ci10\":\n",
    "        cdat = (\n",
    "            torchvision.datasets.CIFAR10\n",
    "        )  # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]) #transform = transforms.Compose([transforms.ToTensor(), norm])\n",
    "        cfg[\"imCh\"] = 3\n",
    "    elif dname == \"Ci100\":\n",
    "        cdat = (\n",
    "            torchvision.datasets.CIFAR100\n",
    "        )  # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        cfg[\"imCh\"] = 3\n",
    "    elif dname == \"Fash\":\n",
    "        cdat = torchvision.datasets.FashionMNIST\n",
    "        # img = img - np.array([0.281])            img = img / np.array([0.352])\n",
    "        trans = transforms.Compose([transforms.ToTensor(), refu])\n",
    "        cfg[\"imCh\"] = 1\n",
    "    elif dname == \"MNIST\":\n",
    "        cdat = torchvision.datasets.MNIST\n",
    "        # trans = transforms.Compose([transforms.ToTensor(), refu, transforms.Normalize(0.1307,0.3081)])\n",
    "        trans = transforms.Compose([transforms.ToTensor(), refu])\n",
    "        cfg[\"imCh\"] = 1\n",
    "    elif dname == \"TinyImgNet\":\n",
    "        cdat = tinyImgNet\n",
    "        norm = getnorm(dname)\n",
    "        # gamma_corr = lambda img : transforms.functional.adjust_gamma(img, gamma=, float=)\n",
    "        # resize_tr = random.choice([transforms.RandomResizedCrop(56), transforms.CenterCrop(56)])\n",
    "        # aug_tr = random.choice([transforms.RandomHorizontalFlip(), transforms.RandomAffine(30, translate=10, scale = [0.8])])\n",
    "        # data_augs = [transforms.RandomResizedCrop(32), transforms.RandomCrop(32), transforms.RandomHorizontalFlip(), transforms.RandomRotation(45)]\n",
    "        train_trans = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=norm[0], std=norm[1]),\n",
    "                transforms.RandomResizedCrop(64),\n",
    "                # transforms.CenterCrop(56),\n",
    "                # transforms.Resize(32),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "            ]\n",
    "        )\n",
    "        val_trans = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.CenterCrop(56),\n",
    "                transforms.Normalize(mean=norm[0], std=norm[1]),\n",
    "                # transforms.Resize(32)\n",
    "            ]\n",
    "        )\n",
    "        cfg[\"imCh\"] = 3\n",
    "\n",
    "    ntrain, down = cfg[\"ntrain\"], True\n",
    "    if dname == \"TinyImgNet\":\n",
    "        down = False\n",
    "        if reconstr == True:\n",
    "            trainset = cdat(\n",
    "                root=\"data/\",\n",
    "                train=True,\n",
    "                download=down,\n",
    "                transform=transforms.Compose([transforms.ToTensor()]),\n",
    "            )\n",
    "            valset = cdat(\n",
    "                root=\"data/\",\n",
    "                train=False,\n",
    "                download=down,\n",
    "                transform=transforms.Compose([transforms.ToTensor()]),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            trainset = cdat(\n",
    "                root=\"data/\", train=True, download=down, transform=train_trans\n",
    "            )\n",
    "            valset = cdat(root=\"data/\", train=False, download=down, transform=val_trans)\n",
    "        train_loader = DataLoader(\n",
    "            trainset, batch_size=cfg[\"batchSize\"], shuffle=True, num_workers=4\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            valset, batch_size=cfg[\"batchSize\"], shuffle=False, num_workers=4\n",
    "        )\n",
    "        return train_loader, val_loader, None, trainset, valset\n",
    "\n",
    "    def loadStore(isTrain, ndat):\n",
    "        nonlocal cdat\n",
    "        trainset = cdat(root=\"data/\", train=isTrain, download=down, transform=trans)\n",
    "        train_dataset = torch.utils.data.DataLoader(\n",
    "            trainset, batch_size=ndat, num_workers=4\n",
    "        )  # cfg[\"batchSize\"]\n",
    "        ds = next(iter(train_dataset))\n",
    "        X, Y = ds[0].clone().numpy(), ds[1].clone().numpy()\n",
    "        # normA = lambda bx: (bx - np.min(bx)) / (np.max(bx) - np.min(bx) + 1e-10)  # \"Normalizing - necessary if no BN\"\n",
    "        # X,Y = normA(X), normA(Y)\n",
    "        ds = [X, Y]\n",
    "        print(\n",
    "            \"Data stats\",\n",
    "            cdat,\n",
    "            X.shape,\n",
    "            np.mean(X, axis=(0, 2, 3)),\n",
    "            np.std(X, axis=(0, 2, 3)),\n",
    "            np.max(X),\n",
    "            np.min(X),\n",
    "            \" Data should be normalized\",\n",
    "        )\n",
    "        ds = sklearn.utils.shuffle(*ds)\n",
    "        return ds[0].astype(np.float16), ds[1].astype(np.int16)\n",
    "\n",
    "    trX, trY = loadStore(True, ntrain)\n",
    "    teX, teY = loadStore(False, ntrain // 2)\n",
    "\n",
    "    def cds(trX, trY, shuffle=True):\n",
    "        ds = TensorDataset(torch.from_numpy(trX), torch.from_numpy(trY))\n",
    "        return DataLoader(\n",
    "            ds, batch_size=cfg[\"batchSize\"], shuffle=shuffle, num_workers=4, pin_memory = True\n",
    "        )\n",
    "\n",
    "    return cds(trX, trY), cds(teX, teY, False), None\n",
    "\n",
    "\n",
    "def tinyImgNet(root: str, train: bool, download: bool, transform) -> Dataset:\n",
    "    filter_val = train and not download\n",
    "\n",
    "    if download:\n",
    "        os.makedirs(root, exist_ok=True)\n",
    "        r = requests.get(\n",
    "            \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\", stream=True\n",
    "        )\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(root)\n",
    "\n",
    "    main_dir_path = os.path.join(root, \"tiny-imagenet-200\")\n",
    "    train_path = os.path.join(main_dir_path, \"train\")\n",
    "    val_path = os.path.join(main_dir_path, \"val\")\n",
    "    if not (os.path.exists(train_path) and os.path.exists(val_path)):\n",
    "        raise ValueError(\"Train and Val paths don't exist\")\n",
    "\n",
    "    val_formatted = os.path.join(main_dir_path, \"val_formatted\")\n",
    "    if not filter_val and not os.path.exists(val_formatted):\n",
    "        with open(os.path.join(val_path, \"val_annotations.txt\"), \"r\") as f:\n",
    "            img_class_map = f.readlines()\n",
    "        for img_class in img_class_map:\n",
    "            img_name, class_name = img_class.split(\"\\t\")[0], img_class.split(\"\\t\")[1]\n",
    "            img_path = os.path.join(\"images\", img_name)\n",
    "            class_dir = os.path.join(val_formatted, class_name)\n",
    "            os.makedirs(os.path.join(class_dir, \"images\"), exist_ok=True)\n",
    "            copyfile(\n",
    "                os.path.join(val_path, img_path), os.path.join(class_dir, img_path)\n",
    "            )\n",
    "    if train:\n",
    "        return ImageFolder(train_path, transform)\n",
    "    else:\n",
    "        return ImageFolder(val_formatted, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cl2zN8eGigd_"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def decay(opt,epoch,optimizerCl):\n",
    "    if opt[0] == \"S\" and (epoch + 1) % (opt[1] // 3+opt[1]//10+2 ) == 0:\n",
    "        for p in optimizerCl.param_groups: p['lr'] *= 0.1\n",
    "        #print(\"  D\", np.round(optimizerCl.param_groups[0]['lr'],5))\n",
    "\n",
    "def getAcc(net, dataset,  niter=10000,norm=None):\n",
    "    correct,total = 0,0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for cit,data in enumerate(dataset):\n",
    "            with tca.autocast():\n",
    "                dsx,dsy = data[0].cuda(),data[1].cuda()\n",
    "                if len(dsx.size()) == 5:\n",
    "                    dsx = dsx.squeeze(0)\n",
    "                    dsy = dsy.squeeze(0)\n",
    "                total += dsy.size(0)\n",
    "                outputs = net(dsx.float())\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += torch.eq(predicted, dsy).sum().item()\n",
    "                if cit>=niter: break\n",
    "    return correct/total\n",
    "\n",
    "\n",
    "def getclassifier(cfg,train_dataset,val_dataset,norm=None):\n",
    "\n",
    "    netCl=ResNet10(n_classes = cfg[\"ds\"][1], n_output_planes = [64, 128, 256, 512], inch = cfg[\"imCh\"]).cuda()\n",
    "    optimizerCl = optim.SGD(netCl.parameters(), lr=cfg[\"opt\"][2], momentum=0.9, weight_decay=cfg[\"opt\"][3])\n",
    "    #optimizerCl = optim.SGD(netCl.parameters(), lr=cfg[\"opt\"][2], momentum=0.9, weight_decay=cfg[\"opt\"][3])\n",
    "    closs,teaccs,trep,loss,clr = 0,[],cfg[\"opt\"][1],nn.CrossEntropyLoss(), cfg[\"opt\"][2]\n",
    "    print(\"Train Classifier to explain\")\n",
    "    scaler = tca.GradScaler()\n",
    "    teAccs,trAccs=[],[]\n",
    "    clAcc = lambda dataset: getAcc(netCl, dataset,  niter=1e10,norm=norm)\n",
    "    for epoch in tqdm(range(trep)):\n",
    "        netCl.train()\n",
    "        for i, data in enumerate(tqdm(train_dataset, position=0, leave=True)):\n",
    "            with tca.autocast():\n",
    "                optimizerCl.zero_grad()\n",
    "                dsx = data[0]\n",
    "                dsx,dsy = dsx.cuda(),data[1].cuda()\n",
    "                if len(dsx.size()) == 5:\n",
    "                    dsx = dsx.squeeze(0)\n",
    "                    dsy = dsy.squeeze(0)\n",
    "                output = netCl(dsx.float())\n",
    "                errD_real = loss(output, dsy.long())\n",
    "                scaler.scale(errD_real).backward()\n",
    "                scaler.step(optimizerCl)\n",
    "                scaler.update()\n",
    "            closs = 0.97 * closs + 0.03 * errD_real.item() if i > 20 else 0.8 * closs + 0.2 * errD_real.item()\n",
    "        decay(cfg[\"opt\"],epoch,optimizerCl)\n",
    "        netCl.eval()\n",
    "        teAccs.append(clAcc(val_dataset))\n",
    "        if (epoch % 4 == 0 and epoch<=13) or (epoch % 20==0 and epoch>13):\n",
    "            print(epoch, np.round(np.array([closs, teAccs[-1], clAcc(train_dataset)]), 5))\n",
    "    lcfg = {\"testAcc\": clAcc(val_dataset), \"trainAcc\": clAcc(train_dataset)}\n",
    "    netCl.eval()\n",
    "    return netCl, lcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0sFhT_vJ4fHX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.cuda.amp as tca\n",
    "import torch\n",
    "from torch import nn\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class ClaDecNet(nn.Module):\n",
    "    def __init__(self, cfg, inShape, nFea):\n",
    "        super(ClaDecNet, self).__init__()\n",
    "        self.channel_mult = int(64)\n",
    "        self.expLinLay = len(inShape) == 2  # linear layer...\n",
    "\n",
    "        dim = 64 if self.expLinLay else 64 // int(inShape[-1])  # dimension of input\n",
    "        self.input_dim = np.prod(inShape[1:])\n",
    "        self.inFea = inShape[-2] if not self.expLinLay else 1\n",
    "        nLay = int(np.round(np.log2(dim) - 2))\n",
    "\n",
    "        # Use batchnorm or bias? LeakyRelu or Relu   -->  Does not make much of a difference\n",
    "        # bn,bias = lambda x:  nn.BatchNorm2d(x),False\n",
    "        # rel =lambda x:  nn.LeakyReLU(0.01)\n",
    "        bn, bias = lambda x: nn.Identity(), True\n",
    "        rel = lambda x: nn.ReLU()\n",
    "\n",
    "        if (\n",
    "            self.inFea == 1\n",
    "        ):  # There is no spatial dimension (or it is one) -> use a dense layer as the first layer\n",
    "            self.useDense = True\n",
    "            self.fc_output_dim = max(\n",
    "                self.input_dim, self.channel_mult\n",
    "            )  # number of input features\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(self.input_dim, self.fc_output_dim), nn.ReLU(True)\n",
    "            )  # , nn.BatchNorm1d(self.fc_output_dim)\n",
    "        else:  # The spatial extend is larger one, use a conv layer, otherwise have too many parameters\n",
    "            self.fc_output_dim = inShape[1]  # number of input features\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Conv2d(inShape[1], inShape[1], 3, stride=1, padding=1, bias=bias),\n",
    "                nn.ReLU(True),\n",
    "            )  # , nn.BatchNorm1d(self.fc_output_dim)\n",
    "            self.useDense = False\n",
    "\n",
    "        temp_nLay = 0\n",
    "        if nLay < 0:\n",
    "            temp_nLay = nLay\n",
    "            nLay = 0\n",
    "        \n",
    "        self.deconv = [\n",
    "            nn.ConvTranspose2d(\n",
    "                self.fc_output_dim,\n",
    "                self.channel_mult * (2 ** nLay),\n",
    "                4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            bn(self.channel_mult * (2 ** nLay)),\n",
    "            rel(None),\n",
    "        ]\n",
    "        if temp_nLay < 0:\n",
    "            self.deconv.append(nn.MaxPool2d((2,2), stride=2))\n",
    "        for j in range(nLay, 0, -1):\n",
    "            self.deconv += [\n",
    "                nn.ConvTranspose2d(\n",
    "                    self.channel_mult * (2 ** j),\n",
    "                    self.channel_mult * (2 ** (j - 1)),\n",
    "                    4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    bias=bias,\n",
    "                ),\n",
    "                bn(self.channel_mult * (2 ** (j - 1))),\n",
    "                rel(None),\n",
    "            ]\n",
    "        self.deconv.append(\n",
    "            nn.ConvTranspose2d(\n",
    "                self.channel_mult * 1, nFea, 4, stride=2, padding=1, bias=True\n",
    "            )\n",
    "        )\n",
    "        if temp_nLay < -1:\n",
    "            self.deconv.append(nn.MaxPool2d((2,2), stride=2))\n",
    "        self.deconv = nn.Sequential(*self.deconv)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.useDense:\n",
    "            x = x.view(-1, self.input_dim)\n",
    "            x = self.fc(x)\n",
    "            x = x.view(-1, self.fc_output_dim, self.inFea, self.inFea)\n",
    "        else:\n",
    "            x = self.fc(x)\n",
    "        x = self.deconv(x)\n",
    "        x = self.sig(\n",
    "            x\n",
    "        )  # This sometimes gives better visualization, but you need to take care to standardize inputs as well\n",
    "        return x\n",
    "\n",
    "\n",
    "def getClaDec(cfg, netCl, norm, train_dataset):\n",
    "    alpha = cfg[\"alpha\"]\n",
    "    closs, cclloss, crloss, teaccs, trep, clloss, clr = (\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        [],\n",
    "        cfg[\"opt\"][1],\n",
    "        nn.CrossEntropyLoss(),\n",
    "        cfg[\"opt\"][2],\n",
    "    )\n",
    "    print(\"Train CLaDec\")\n",
    "    scaler = tca.GradScaler()\n",
    "    d = next(iter(train_dataset))\n",
    "    netDec = ClaDecNet(cfg, d[2].squeeze(0).shape, cfg[\"imCh\"]).cuda()\n",
    "    netDec.train()\n",
    "    netCl.train()\n",
    "    optimizerCl = torch.optim.Adam(netDec.parameters(), lr=0.0003, weight_decay=1e-5)\n",
    "    aeloss = nn.MSELoss()\n",
    "    ulo = (\n",
    "        lambda closs, totloss, i: 0.97 * closs + 0.03 * totloss.item()\n",
    "        if epoch > 20\n",
    "        else 0.8 * closs + 0.2 * totloss.item()\n",
    "    )\n",
    "    for epoch in tqdm(range(trep)):\n",
    "        for i, data in enumerate(train_dataset):\n",
    "            with tca.autocast():\n",
    "                optimizerCl.zero_grad()\n",
    "#                 if cfg[\"ds\"][0] == \"TinyImgNet\":\n",
    "#                     dsx, dsy, dsact = (\n",
    "#                         data[0].squeeze(0).cuda(),\n",
    "#                         data[1].squeeze(0).cuda(),\n",
    "#                         data[2].squeeze(0).cuda(),\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     dsx, dsy, dsact = data[0].cuda(), data[1].cuda(), data[2].cuda()\n",
    "                dsx, dsy, dsact = (\n",
    "                    data[0].squeeze(0).cuda(),\n",
    "                    data[1].squeeze(0).cuda(),\n",
    "                    data[2].squeeze(0).cuda(),\n",
    "                )\n",
    "\n",
    "                output = netDec(dsact.float())\n",
    "                recloss = aeloss(output, dsx)\n",
    "                if cfg[\"ds\"][0] == \"TinyImgNet\":\n",
    "                    norm = getnorm(\"TinyImgNet\")\n",
    "                    transform = transforms.Compose(\n",
    "                        [\n",
    "                            transforms.Normalize(mean=norm[0], std=norm[1]),\n",
    "                            transforms.RandomResizedCrop(64),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                        ]\n",
    "                    )\n",
    "                    claloss = clloss(netCl(transform(output)), dsy.long())\n",
    "                else:\n",
    "                    claloss = clloss(netCl(output), dsy.long())\n",
    "                totloss = (1 - alpha) * recloss + alpha * claloss\n",
    "                scaler.scale(totloss).backward()\n",
    "                scaler.step(optimizerCl)\n",
    "                scaler.update()\n",
    "                closs, cclloss, crloss = (\n",
    "                    ulo(closs, totloss, epoch),\n",
    "                    ulo(cclloss, claloss, epoch),\n",
    "                    ulo(crloss, recloss, epoch),\n",
    "                )\n",
    "\n",
    "        decay(cfg[\"opt\"], epoch, optimizerCl)\n",
    "        if (epoch % 2 == 0 and epoch <= 10) or (epoch % 10 == 0 and epoch > 10):\n",
    "            print(epoch, np.round(np.array([closs, crloss, cclloss]), 5))\n",
    "\n",
    "    lcfg = {\"ClaLo\": closs}\n",
    "    netDec.eval()\n",
    "    return netDec, lcfg\n",
    "\n",
    "\n",
    "def getActModel(cfg, classifier):\n",
    "    ind = cfg[\"layInd\"]\n",
    "    if cfg[\"ds\"][0] == \"TinyImgNet\":\n",
    "        if ind < -1:\n",
    "            ind = ind - 3\n",
    "    actModel = nn.Sequential(*list(classifier.children())[:ind])\n",
    "    actModel.eval()\n",
    "    return actModel\n",
    "\n",
    "\n",
    "class RefAE(nn.Module):\n",
    "    def __init__(self, cfg, inShape):\n",
    "        super(RefAE, self).__init__()\n",
    "        self.cladec = ClaDecNet(cfg, inShape, cfg[\"imCh\"])\n",
    "        self.cladec.train()\n",
    "        cla = ResNet10(n_classes = cfg[\"ds\"][1], n_output_planes = [64, 128, 256, 512], inch = cfg[\"imCh\"]).cuda()\n",
    "        actModel = getActModel(cfg, cla)\n",
    "        actModel.train()\n",
    "        self.seq = nn.Sequential(actModel, self.cladec)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "\n",
    "def getRefAE(cfg, trds, train_dataset):\n",
    "    closs, teaccs, trep, clloss, clr = (\n",
    "        0,\n",
    "        [],\n",
    "        cfg[\"opt\"][1],\n",
    "        nn.CrossEntropyLoss(),\n",
    "        cfg[\"opt\"][2],\n",
    "    )\n",
    "    print(\"Train RefAE\")\n",
    "    scaler = tca.GradScaler()\n",
    "    d = next(iter(trds))\n",
    "    netDec = RefAE(cfg, d[2].squeeze(0).shape).cuda()\n",
    "    netDec.train()\n",
    "    optimizerCl = torch.optim.Adam(netDec.parameters(), lr=0.0003, weight_decay=1e-5)\n",
    "    aeloss = nn.MSELoss()\n",
    "    ulo = (\n",
    "        lambda closs, totloss, i: 0.97 * closs + 0.03 * totloss.item()\n",
    "        if epoch > 20\n",
    "        else 0.8 * closs + 0.2 * totloss.item()\n",
    "    )\n",
    "    for epoch in tqdm(range(trep)):\n",
    "        for i, data in enumerate(train_dataset):\n",
    "            with tca.autocast():\n",
    "                optimizerCl.zero_grad()\n",
    "#                 if cfg[\"ds\"][0] == \"TinyImgNet\":\n",
    "#                     dsx = data[0].squeeze(0).cuda()\n",
    "#                 else:\n",
    "#                     dsx = data[0].cuda()\n",
    "                dsx = data[0].squeeze(0).cuda()\n",
    "                output = netDec(dsx.float())\n",
    "                recloss = aeloss(output, dsx)\n",
    "                scaler.scale(recloss).backward()\n",
    "                scaler.step(optimizerCl)\n",
    "                scaler.update()\n",
    "                closs = ulo(closs, recloss, epoch)\n",
    "\n",
    "        decay(cfg[\"opt\"], epoch, optimizerCl)\n",
    "        if (epoch % 2 == 0 and epoch <= 10) or (epoch % 10 == 0 and epoch > 10):\n",
    "            print(epoch, np.round(np.array([closs]), 5))\n",
    "\n",
    "    lcfg = {\"RefAELo\": closs}\n",
    "    netDec.eval()\n",
    "    return netDec, lcfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e2f1WIQ54nIN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,TensorDataset\n",
    "import numpy as np,os\n",
    "import torch.cuda.amp as tca\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def showPics(pics, mtit=\"\", tits =None,nrows = 16,ncols=12,fname=\"expPics\"):\n",
    "    picPerFig = nrows * ncols\n",
    "    for i in range(max(1,pics.shape[0] // picPerFig)):\n",
    "        fig = plt.figure(figsize=(20,30))\n",
    "        wm = plt.get_current_fig_manager()\n",
    "        wm.resize(*wm.window.maxsize())  # wm.window.state('zoomed')\n",
    "        fig.suptitle(mtit, fontsize=8)\n",
    "        for j in range(picPerFig):\n",
    "            if i * picPerFig + j == pics.shape[0]: break\n",
    "            ax1 = plt.subplot(nrows, (picPerFig - 1) // nrows + 1, j + 1)\n",
    "            if not tits is None and i*picPerFig + j<len(tits):\n",
    "               tit=ax1.set_title(str(tits[i*picPerFig + j]), fontsize=16)\n",
    "               plt.setp(tit, color='black')\n",
    "            cpic=pics[i * picPerFig + j].squeeze()\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_yticklabels([])\n",
    "            ax1.axis('off')\n",
    "            cpic = 1-(cpic-np.min(cpic))/(np.max(cpic)-np.min(cpic)+1e-10)\n",
    "            plt.imshow(cpic.astype(np.float32),cmap='Greys')\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        plt.savefig(fname+str(i)+ \".png\")\n",
    "        plt.close()\n",
    "        if i * picPerFig + j >= pics.shape[0] - 1: break\n",
    "\n",
    "def getActs(ds,actModel,cfg):\n",
    "        acts=[]\n",
    "        X,y=[],[]\n",
    "        for i, data in enumerate(ds):\n",
    "            with tca.autocast():\n",
    "                dsx, dsy = data[0].cuda(), data[1].cuda()\n",
    "                X.append(data[0])\n",
    "                y.append(data[1])\n",
    "                #classifier(dsx)\n",
    "                acts.append(actModel(dsx).detach().cpu())\n",
    "        X=torch.cat(X,dim=0)\n",
    "        y=torch.cat(y, dim=0)\n",
    "        conacts=torch.cat(acts,dim=0)\n",
    "        dsact=TensorDataset(X,y,conacts)\n",
    "        return torch.utils.data.DataLoader(dsact, batch_size=cfg[\"batchSize\"], shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 929,
     "referenced_widgets": [
      "e061b591c44840d7bfc95f7868161920",
      "91e126cc6cc5453c8090bbff4bef93bb",
      "44dc577cb11146fda612deb817dd9e2f",
      "762bfde8ede74d2e9bfffdb1d0e32496",
      "6970411b2df34503ad2486672bf1cf10",
      "9d2c38bb74784716bfd1b1adc2b7cfc3",
      "f88df70769a84eaeb138fa3f93ab7bb4",
      "b2c30f83d4f44502a8a8ef35b01d6d07",
      "6314643e016f46f58d3227069c4285f4",
      "8b5294c3f4ad45e292f6e601150e7fb7",
      "8bdc42fe22a44a96ba00666758dfeeeb",
      "7655f32e1c6544b0bc85de272d43ba36",
      "99cbea3befbe4e618fe4a44efcb8b4d3",
      "59a0f3638bf241348a2aa6162b69db60",
      "f7a75a7b81564cc3ad8531a263dc5640",
      "ebdf382cb4fc4a89ab57e48169ca1509",
      "a4999b421a1047d8828791c5dacc63bf",
      "aae552c82396469c9a23f40d46ba9e78",
      "cbb7bd015c114720a7c5c4aea885415e",
      "d969f77bc761449aa684e69df606cf6d",
      "d45f2fcd53bb487fb359906c4e607f8d",
      "3f9f7c15b880430ba8a5f02d7f9cda9e",
      "b86844bde02044808c130622bc44dcc7",
      "256b279d03d641fca6f05748506afb2a",
      "0fe934bf02294b659aab87d16f2e1bf3",
      "ff36e47b2e1f4397b0a934d4c42f1e5a",
      "c6370c2436f843b89240ce693b58f4dd",
      "0c22cb3b195d49dab6d705a905f18142",
      "3ef6458597e847d185383a25357af0b7",
      "3cdbbe1ee106490084ade12486add797",
      "dd503faeeef742eab52e87ae7b067695",
      "3d2fb2afbaf64e2c84e336f6e332b754"
     ]
    },
    "id": "IOPfBQf_4use",
    "outputId": "5dfae529-29f6-4e42-ce99-f05b86e0b67e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing config {'ds': ('MNIST', 10), 'batchSize': 128, 'opt': ('S', 60, 0.1, 0.0001), 'layInd': -1, 'alpha': 0.001, 'ntrain': 60000}\n",
      "Get dataset\n",
      "Data stats <class 'torchvision.datasets.mnist.MNIST'> (60000, 1, 64, 64) [0.1308931] [0.30838418] 1.0 0.0  Data should be normalized\n",
      "Data stats <class 'torchvision.datasets.mnist.MNIST'> (10000, 1, 64, 64) [0.13275763] [0.31077635] 1.0 0.0  Data should be normalized\n"
     ]
    }
   ],
   "source": [
    "dummy=False\n",
    "#dummy = True\n",
    "#cfg={ 'ds': ('Fash', 10),  #Dataset either  ('Ci100', 100) or ('Ci10', 10)\n",
    "#      'batchSize': 128, 'opt': ('S', 1 if dummy else 64, 0.1, 0.0001), #optimizer settings\n",
    "#      'layInd':-1, #Layer to explain (from last layer back, ie. -1 is last (linear), -2 is last conv, -3 second last conv)\n",
    "#      'alpha': 0.001, #tradeoff parameter reconstruction vs. classification loss\n",
    "#      'ntrain': 500 if dummy else 60000}\n",
    "cfg={ 'ds': ('MNIST', 10),  #Dataset either  ('Ci100', 100) or ('Ci10', 10)\n",
    "    'batchSize': 128, 'opt': ('S', 1 if dummy else 60, 0.1, 0.0001), #optimizer settings\n",
    "    'layInd':-1, #Layer to explain (from last layer back, ie. -1 is last (linear), -2 is last conv, -3 second last conv)\n",
    "    'alpha': 0.001, #tradeoff parameter reconstruction vs. classification loss\n",
    "    'ntrain': 500 if dummy else 60000}\n",
    "print(\"Executing config\",cfg)\n",
    "cfg[\"num_classes\"]=cfg[\"ds\"][1]\n",
    "#Get Data\n",
    "print(\"Get dataset\")\n",
    "train_dataset, val_dataset,norm=getFullDS(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d2-jqGXi-Tfn"
   },
   "outputs": [],
   "source": [
    "model_path = \"trained_models/resnet/MNIST\"\n",
    "import os\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toUmp-Fs_q_5",
    "outputId": "bb4c34a5-3b5a-4736-9181-72b50ff63633"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa5omGjUKSAU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZUhQiZ64-RP",
    "outputId": "7f8489d7-314d-44c6-99dd-a5d060706fac"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train and save non-reflective Model\n",
    "cfg[\"opt\"] = ('S', 1 if dummy else 20, 0.1, 0.0001)\n",
    "classifier, lcfg = getclassifier(cfg,  train_dataset, val_dataset, norm=norm)\n",
    "print(\"Classifier Accuracy\",lcfg)\n",
    "torch.save(classifier.state_dict(), os.path.join(model_path, \"classifier.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zV2ViQSUVk56",
    "outputId": "9f814281-1524-4924-911d-4c726335b03e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ResNet10(n_classes = cfg[\"ds\"][1], n_output_planes = [64, 128, 256, 512], inch = cfg[\"imCh\"])\n",
    "classifier.load_state_dict(torch.load(os.path.join(model_path, \"classifier.pt\")))\n",
    "classifier.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "586CzeDyAiN-"
   },
   "outputs": [],
   "source": [
    "def getReconstrDS(acts_ds,refAE,cladecNet,cfg):\n",
    "    X_cladec,X_refae,y=[],[],[]\n",
    "    cladec_loss, refae_loss = 0, 0\n",
    "    for i, data in enumerate(acts_ds):\n",
    "        #if i > 2: break\n",
    "        with tca.autocast():\n",
    "            with torch.no_grad():\n",
    "                dsx, dsy,dsact = data[0].cuda(), data[1], data[2].cuda()\n",
    "                outCla = cladecNet(dsact).detach().cpu().float()\n",
    "                outAE = refAE(dsx).detach().cpu().float()\n",
    "                dsx = dsx.detach().squeeze().cpu().float()\n",
    "                cladec_loss += (outCla - dsx).pow(2).mean()\n",
    "                refae_loss += (outAE - dsx).pow(2).mean()\n",
    "                X_cladec.append(outCla)\n",
    "                X_refae.append(outAE)\n",
    "                y.append(dsy)\n",
    "    \n",
    "    \n",
    "    refae_loss = refae_loss/i\n",
    "    cladec_loss = cladec_loss/i\n",
    "    X_cladec = torch.cat(X_cladec, dim = 0)\n",
    "    X_refae = torch.cat(X_refae, dim = 0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "    cladec_ds = TensorDataset(X_cladec, y)\n",
    "    refae_ds = TensorDataset(X_refae, y)\n",
    "    \n",
    "    cladec_loader = torch.utils.data.DataLoader(cladec_ds, batch_size=cfg[\"batchSize\"], shuffle=True, num_workers=4)\n",
    "    refae_loader = torch.utils.data.DataLoader(refae_ds, batch_size=cfg[\"batchSize\"], shuffle=True, num_workers=4)\n",
    "    return cladec_loader, refae_loader, cladec_loss, refae_loss        \n",
    "        #X=torch.cat(X,dim=0)\n",
    "        #y=torch.cat(y, dim=0)\n",
    "        #dsact=TensorDataset(X,y,conacts)\n",
    "        #return torch.utils.data.DataLoader(dsact, batch_size=cfg[\"batchSize\"], shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUERMR82BNgP"
   },
   "outputs": [],
   "source": [
    "batch_ordered = torch.load(os.path.join(model_path, \"original.pt\"))\n",
    "clf_preds = torch.load(os.path.join(model_path,\"clf_preds.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRbCKhTMBNr7"
   },
   "outputs": [],
   "source": [
    "batch_ordered = [x for x in val_dataset]\n",
    "torch.save(batch_ordered,os.path.join(model_path, \"original.pt\"))\n",
    "clf_preds = []\n",
    "for (x,y) in batch_ordered:\n",
    "    with torch.no_grad():\n",
    "        with tca.autocast():\n",
    "            clf_preds.append(classifier(x.cuda()).argmax(dim=1).detach().cpu())\n",
    "torch.save(clf_preds, os.path.join(model_path,\"clf_preds.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, IterableDataset\n",
    "import models as clModel,dutils\n",
    "import cladec\n",
    "import numpy as np,os\n",
    "import torch.cuda.amp as tca\n",
    "from matplotlib import pyplot as plt\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "class LargeActDataset(IterableDataset):\n",
    "    def __init__(self, orig_loader, act_model, device):\n",
    "        super(LargeActDataset, self).__init__()\n",
    "        self.orig_loader = orig_loader\n",
    "        self.act_model = act_model\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __iter__(self) :\n",
    "        for (img, label) in self.orig_loader:\n",
    "            with torch.no_grad():\n",
    "                with tca.autocast():\n",
    "                    act = self.act_model(img.to(self.device)).detach().cpu()\n",
    "            yield (img.detach().cpu(), label, act)\n",
    "\n",
    "\n",
    "def getActs(ds,actModel,cfg):\n",
    "    dsact = LargeActDataset(ds, actModel, cfg[\"device\"])\n",
    "    return torch.utils.data.DataLoader(dsact, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 50/469 [00:01<00:12, 34.40it/s]\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for daat in tqdm(trds):\n",
    "    j+=1\n",
    "    if j > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"device\"] = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYIS1whm8eZ3",
    "outputId": "c9dc58d7-b397-4d4e-b1cf-a6278de56887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RefAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [01:02<1:01:41, 62.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.00018]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/60 [03:07<59:15, 62.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [0.0001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/60 [05:11<57:05, 62.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [4.e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/60 [07:16<55:01, 62.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [2.e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9/60 [09:20<52:57, 62.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 [4.e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [11:25<50:55, 62.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [5.e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [21:50<40:37, 62.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 [2.e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 31/60 [32:15<30:10, 62.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 [1.e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 41/60 [42:39<19:46, 62.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 [1.e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [53:03<09:21, 62.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 [1.e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [1:02:25<00:00, 62.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RefAELo': 6.168587099979602e-06}\n",
      "Train CLaDec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 1/60 [02:22<2:20:35, 142.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.00035 0.00035 0.00111]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/60 [07:08<2:15:34, 142.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [0.00017 0.00017 0.00093]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/60 [11:53<2:10:52, 142.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [0.00011 0.00011 0.00095]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/60 [16:40<2:06:16, 142.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [0.0001  0.0001  0.00105]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9/60 [21:25<2:01:28, 142.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 [8.0e-05 8.0e-05 8.4e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [26:11<1:56:40, 142.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [7.0e-05 7.0e-05 7.1e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [50:03<1:32:58, 143.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 [6.00e-05 5.00e-05 1.08e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 23/60 [54:50<1:28:19, 143.23s/it]"
     ]
    }
   ],
   "source": [
    "ae_train_loss_list = []\n",
    "ae_val_loss_list = []\n",
    "cladec_train_loss_list = []\n",
    "cladec_val_loss_list = []\n",
    "cladec_val_acc = []\n",
    "refae_val_acc = []\n",
    "\n",
    "for i in range(4, 5):\n",
    "    cfg[\"opt\"] = ('S', 1 if dummy else 60, 0.1, 0.0001)\n",
    "    refae_path = os.path.join(model_path, \"refAE_{}.pt\".format(str(i)))\n",
    "    cladec_path = os.path.join(model_path, \"cladecNet_{}.pt\".format(str(i)))\n",
    "    cfg[\"layInd\"] = -1*i\n",
    "    #get Activations\n",
    "    actModel = getActModel(cfg, classifier)\n",
    "    actModel.eval()\n",
    "    trds=getActs(train_dataset, actModel,cfg)\n",
    "    \n",
    "    if i > 3:\n",
    "        #get RefAE\n",
    "        refAE,rcfg =getRefAE(cfg,trds, train_dataset) #Does not use activations themselves, only needs shape\n",
    "        print(rcfg)\n",
    "        torch.save(refAE.state_dict(), refae_path)\n",
    "    # Train ClaDec\n",
    "    cladecNet,ccfg=  getClaDec(cfg,classifier,None,trds)\n",
    "    print(\"ClaDec Final loss\", ccfg)\n",
    "    torch.save(cladecNet.state_dict(), cladec_path)\n",
    "    \n",
    "    # d = next(iter(trds))\n",
    "    # refAE = RefAE(cfg, d[2].squeeze(0).shape).cuda()\n",
    "    # refAE.load_state_dict(torch.load(refae_path))\n",
    "    \n",
    "    # cladecNet = ClaDecNet(cfg, d[2].squeeze(0).shape, cfg[\"imCh\"]).cuda()\n",
    "    # refAE.eval()\n",
    "    # cladecNet.eval()\n",
    "    # actModel.eval()\n",
    "    \n",
    "    # train_acts = getActs(train_dataset, actModel, cfg)\n",
    "    # val_acts = getActs(val_dataset, actModel, cfg)\n",
    "    \n",
    "    # cladec_train, refae_train, cladec_train_loss, refae_train_loss = getReconstrDS(train_acts, refAE, cladecNet, cfg)\n",
    "    # cladec_val, refae_val, cladec_val_loss, refae_val_loss = getReconstrDS(train_acts, refAE, cladecNet, cfg)\n",
    "    \n",
    "    # ae_train_loss_list.append(refae_train_loss)\n",
    "    # ae_val_loss_list.append(refae_val_loss)\n",
    "    # cladec_train_loss_list.append(cladec_train_loss)\n",
    "    # cladec_val_loss_list.append(cladec_val_loss)\n",
    "    \n",
    "\n",
    "    # cfg[\"opt\"] = ('S', 1 if dummy else 20, 0.1, 0.0001)\n",
    "    # clf_cladec, lcfg_cladec = getclassifier(cfg,  cladec_train, cladec_val, norm=norm)\n",
    "    # cladec_val_acc.append(lcfg_cladec[\"testAcc\"])\n",
    "    # torch.save(clf_cladec.state_dict(), os.path.join(model_path, \"clf_cladec_\" + str(i) + \".pt\"))\n",
    "    # clf_ae, lcfg_ae = getclassifier(cfg,  refae_train, refae_val, norm=norm)\n",
    "    # refae_val_acc.append(lcfg_ae[\"testAcc\"])\n",
    "    # torch.save(clf_ae.state_dict(), os.path.join(model_path, \"clf_ae_\" + str(i) + \".pt\"))\n",
    "    # torch.save({\"cladec_train_loss\" : cladec_train_loss_list, \"cladec_val_loss\": cladec_val_loss_list, \"cladec_val_acc\" : cladec_val_acc}, os.path.join(model_path,\"cladec_metrics_{}.pt\".format(str(i))))\n",
    "    # torch.save({\"ae_train_loss\" : ae_train_loss_list, \"ae_val_loss\": ae_val_loss_list, \"ae_val_acc\" : refae_val_acc}, os.path.join(model_path, \"ae_metrics_{}.pt\".format(str(i))))\n",
    "\n",
    "\n",
    "\n",
    "    # ae_reconstr = []\n",
    "    # cladec_reconstr = []\n",
    "    # with tca.autocast():\n",
    "    #     for (x,y) in (batch_ordered):\n",
    "    #         y_pred_ae = clf_ae(x.cuda()).argmax(dim=1).detach().cpu()\n",
    "    #         ae_reconstr.append((refAE(x.cuda()).detach().cpu(),y_pred_ae))\n",
    "    #         y_pred_cladec = clf_cladec(x.cuda()).argmax(dim=1).detach().cpu()\n",
    "    #         cladec_reconstr.append((cladecNet(actModel(x.cuda())).detach().cpu(),y_pred_cladec))\n",
    "    \n",
    "    # torch.save(ae_reconstr,os.path.join(model_path, \"ae_reconstr_\" + str(i)+ \".pt\"))\n",
    "    # torch.save(cladec_reconstr,os.path.join(model_path, \"cladec_reconstr_\" + str(i)+ \".pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A64UqBzq8mO3",
    "outputId": "972e62d3-ac42-4edb-90c6-59687037e633"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHfqCbvB69nx"
   },
   "outputs": [],
   "source": [
    "actModel = getActModel(cfg, classifier)\n",
    "actModel.eval()\n",
    "trds=getActs(train_dataset,actModel,cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HncNDLCH8_hW",
    "outputId": "337c5532-4e42-4c90-aaab-5bbc742c9880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RefAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:19<00:19, 19.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.00115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:38<00:00, 19.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RefAELo': 0.0006595195792957057}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "refAE,rcfg =getRefAE(cfg,trds)\n",
    "print(rcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UZRdjyu9Gkh",
    "outputId": "b38265be-c8ca-4420-8611-58d3bfb90c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CLaDec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:19<00:19, 19.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.00438 0.00433 0.05723]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:39<00:00, 19.56s/it]\n"
     ]
    }
   ],
   "source": [
    "cladecNet,ccfg=  getClaDec(cfg,classifier,norm,trds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skimage\n",
      "  Downloading skimage-0.0.tar.gz (757 bytes)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /home/harshv834/my-envs/ml_def/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-khfm7_zb/skimage_fd8c4a1ba1f548ad9d5356e9ee2a06df/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-khfm7_zb/skimage_fd8c4a1ba1f548ad9d5356e9ee2a06df/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-5w1e6jf5\n",
      "         cwd: /tmp/pip-install-khfm7_zb/skimage_fd8c4a1ba1f548ad9d5356e9ee2a06df/\n",
      "    Complete output (3 lines):\n",
      "    \n",
      "    *** Please install the `scikit-image` package (instead of `skimage`) ***\n",
      "    \n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/3b/ee/edbfa69ba7b7d9726e634bfbeefd04b5a1764e9e74867ec916113eeaf4a1/skimage-0.0.tar.gz#sha256=6c96a11d9deea68489c9b80b38fad1dcdab582c36d4fa093b99b24a3b30c38ec (from https://pypi.org/simple/skimage/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement skimage (from versions: 0.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for skimage\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "n4JbPt3D9508"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "def save_reconstructions(cfg, data_loader, model_path, model, train=True, cladec=True):\n",
    "    rec_path = os.path.join(model_path, \"rec_cladec\" if cladec else \"rec_refae\", \"reconstructions_{}\".format(str(-1*cfg[\"layInd\"])))\n",
    "    os.makedirs(rec_path, exist_ok = True)\n",
    "    dset_path = os.path.join(rec_path, \"train\" if train else \"val\")\n",
    "    os.makedirs(dset_path, exist_ok=True)\n",
    "    img_tr = transforms.ToPILImage()\n",
    "    total_classes = set()\n",
    "    idx = 0\n",
    "    for (img, label) in tqdm(data_loader, position=0, leave=True):\n",
    "        label = np.array(label)\n",
    "        if len(total_classes) < cfg[\"ds\"][1]:\n",
    "            label_classes = set(label)\n",
    "            new_classes = label_classes.difference(total_classes)\n",
    "            total_classes = total_classes.union(label_classes)\n",
    "            for i in new_classes:\n",
    "                class_path = os.path.join(dset_path, str(i))\n",
    "                os.makedirs(class_path, exist_ok=True)\n",
    "        \n",
    "        with tca.autocast():\n",
    "            act = model(img.to(cfg[\"device\"])).detach().cpu()\n",
    "        for j in range(len(img)):\n",
    "            idx +=1\n",
    "            vec = act[j].squeeze(0).numpy()\n",
    "#             vec = img_tr(act[j])\n",
    "            vec_label = label[j]\n",
    "            vec_path = os.path.join(dset_path, str(vec_label),str(idx) + \".jpg\")\n",
    "            imageio.imwrite(vec_path,(vec*255.9).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"device\"] = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:48<00:00,  9.73it/s]\n",
      "100%|██████████| 79/79 [00:08<00:00,  9.29it/s]\n",
      "100%|██████████| 469/469 [00:48<00:00,  9.74it/s]\n",
      "100%|██████████| 79/79 [00:08<00:00,  9.32it/s]\n",
      "100%|██████████| 1/1 [01:53<00:00, 113.77s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(2,3)):\n",
    "    cfg[\"layInd\"] = -1*i\n",
    "    actModel = getActModel(cfg, classifier)\n",
    "    actModel.eval()\n",
    "    trds=getActs(train_dataset,actModel,cfg)\n",
    "\n",
    "    d = next(iter(trds))\n",
    "    refAE = RefAE(cfg, d[2].squeeze(0).shape).cuda()\n",
    "    refAE.load_state_dict(torch.load(os.path.join(model_path, \"refAE_\"+  str(i) + \".pt\")))\n",
    "    cladecNet = ClaDecNet(cfg, d[2].squeeze(0).shape, cfg[\"imCh\"]).cuda()\n",
    "    cladecNet.load_state_dict(torch.load(os.path.join(model_path, \"cladecNet_\"+  str(i) + \".pt\")))\n",
    "    refAE.eval()\n",
    "    cladecNet.eval()\n",
    "    cladec_ae = torch.nn.Sequential(actModel, cladecNet)\n",
    "    cladec_ae.eval()\n",
    "    save_reconstructions(cfg, train_dataset, model_path, refAE, train=True, cladec=False)\n",
    "    save_reconstructions(cfg, val_dataset, model_path, refAE, train=False, cladec=False)\n",
    "    save_reconstructions(cfg, train_dataset, model_path, cladec_ae, train=True, cladec=True)\n",
    "    save_reconstructions(cfg, val_dataset, model_path, cladec_ae, train=False, cladec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, IterableDataset\n",
    "import numpy as np,os\n",
    "import torch.cuda.amp as tca\n",
    "from matplotlib import pyplot as plt\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "class LargeActDataset(IterableDataset):\n",
    "    def __init__(self, orig_loader, act_model, device):\n",
    "        super(LargeActDataset, self).__init__()\n",
    "        self.orig_loader = orig_loader\n",
    "        self.act_model = act_model\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __iter__(self) :\n",
    "        for (img, label) in self.orig_loader:\n",
    "            with torch.no_grad():\n",
    "                with tca.autocast():\n",
    "                    act = self.act_model(img.to(self.device)).detach().cpu()\n",
    "            yield (img.detach().cpu(), label, act)\n",
    "\n",
    "def getActs(ds,actModel,cfg):\n",
    "    dsact = LargeActDataset(ds, actModel, cfg[\"device\"])\n",
    "    return torch.utils.data.DataLoader(dsact, \n",
    "                                       batch_size=1)\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class LargeReconstrDataset(IterableDataset):\n",
    "    def __init__(self, orig_loader, model, device, train):\n",
    "        super(LargeReconstrDataset, self).__init__()\n",
    "        self.orig_loader = orig_loader\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        norm = getnorm(\"TinyImgNet\")\n",
    "        tr_list = [transforms.Normalize(mean = norm[0], std=norm[1])]\n",
    "        if train:\n",
    "            tr_list.append(transforms.RandomResizedCrop(64))\n",
    "            tr_list.append(transforms.RandomHorizontalFlip())\n",
    "            \n",
    "        self.transform = transforms.Compose(tr_list)\n",
    "\n",
    "    def __iter__(self) :\n",
    "        for (img, label) in self.orig_loader:\n",
    "            with torch.no_grad():\n",
    "                with tca.autocast():\n",
    "                    reconstr = self.model(img.to(self.device)).detach().cpu()\n",
    "                reconstr = self.transform(reconstr)\n",
    "            yield (reconstr, label)\n",
    "\n",
    "def getReconstr(ds,model,cfg, train):\n",
    "    dsact = LargeReconstrDataset(ds, model, cfg[\"device\"], train)\n",
    "    return torch.utils.data.DataLoader(dsact, batch_size=1)\n",
    "\n",
    "def getReconstrDS(acts_ds,refAE,cladecNet):\n",
    "    cladec_loss, refae_loss = 0, 0\n",
    "    for i, data in enumerate(tqdm(acts_ds, position=0, leave=True)):\n",
    "        #if i > 2: break\n",
    "        with tca.autocast():\n",
    "            with torch.no_grad():\n",
    "                dsx, dsact = data[0].cuda(), data[2].cuda()\n",
    "                if len(dsx.size()) == 5:\n",
    "                    dsx = dsx.squeeze(0)\n",
    "                    dsact = dsact.squeeze(0)\n",
    "                outCla = cladecNet(dsact).detach().cpu().float()\n",
    "                outAE = refAE(dsx).detach().cpu().float()\n",
    "                dsx = dsx.detach().squeeze().cpu().float()\n",
    "                cladec_loss += (outCla - dsx).pow(2).mean()\n",
    "                refae_loss += (outAE - dsx).pow(2).mean()\n",
    "    \n",
    "    \n",
    "    refae_loss = refae_loss/i\n",
    "    cladec_loss = cladec_loss/i\n",
    "    return cladec_loss, refae_loss\n",
    "        #X=torch.cat(X,dim=0)\n",
    "        #y=torch.cat(y, dim=0)\n",
    "        #dsact=TensorDataset(X,y,conacts)\n",
    "        #return torch.utils.data.DataLoader(dsact, batch_size=cfg[\"batchSize\"], shuffle=True, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "from typing import Any, Callable, cast, Dict, List, Optional, Tuple\n",
    "from torchvision.datasets import DatasetFolder\n",
    "def bw_loader(path):\n",
    "    img = Image.open(path)\n",
    "    return img\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "class BWFolder(DatasetFolder):\n",
    "    \"\"\"A generic data loader where the images are arranged in this way by default: ::\n",
    "        root/dog/xxx.png\n",
    "        root/dog/xxy.png\n",
    "        root/dog/[...]/xxz.png\n",
    "        root/cat/123.png\n",
    "        root/cat/nsdf3.png\n",
    "        root/cat/[...]/asd932_.png\n",
    "    This class inherits from :class:`~torchvision.datasets.DatasetFolder` so\n",
    "    the same methods can be overridden to customize the dataset.\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "        is_valid_file (callable, optional): A function that takes path of an Image file\n",
    "            and check if the file is a valid file (used to check of corrupt files)\n",
    "     Attributes:\n",
    "        classes (list): List of the class names sorted alphabetically.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            loader: Callable[[str], Any] = bw_loader,\n",
    "            is_valid_file: Optional[Callable[[str], bool]] = None,\n",
    "    ):\n",
    "        super(BWFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                          transform=transform,\n",
    "                                          target_transform=target_transform,\n",
    "                                          is_valid_file=is_valid_file)\n",
    "        self.imgs = self.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def getReconstr(cfg, model_path, cladec=True, train=True):\n",
    "    rec_path = os.path.join(model_path, \"rec_cladec\" if cladec else \"rec_refae\", \"reconstructions_{}\".format(str(-1*cfg[\"layInd\"])),\"train\" if train else \"val\")\n",
    "    if train:\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.RandomResizedCrop(64), transforms.RandomHorizontalFlip()])\n",
    "    else:\n",
    "        transform = transforms.ToTensor()\n",
    "    ds_reconstr = BWFolder(rec_path, transform = transform)\n",
    "    return torch.utils.data.DataLoader(ds_reconstr, batch_size=cfg[\"batchSize\"], shuffle=train, num_workers = 4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "(img, label) = next(iter(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = img[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(one.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.imwrite(\"one.jpg\", (one.reshape(64,64)*255.9).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"one.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [01:49,  4.27it/s]:00<?, ?it/s]\n",
      "469it [01:48,  4.32it/s]\n",
      " 33%|███▎      | 1/3 [03:38<07:17, 218.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.1327)]\n",
      "[tensor(0.1327)]\n",
      "[tensor(0.1313)]\n",
      "[tensor(0.1312)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [01:53,  4.13it/s]\n",
      "469it [01:53,  4.12it/s]\n",
      " 67%|██████▋   | 2/3 [07:26<03:44, 224.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.1327), tensor(0.1331)]\n",
      "[tensor(0.1327), tensor(0.1331)]\n",
      "[tensor(0.1313), tensor(0.1323)]\n",
      "[tensor(0.1312), tensor(0.1323)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [02:29,  3.13it/s]\n",
      "469it [02:28,  3.15it/s]\n",
      "100%|██████████| 3/3 [12:25<00:00, 248.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.1327), tensor(0.1331), tensor(0.1333)]\n",
      "[tensor(0.1327), tensor(0.1331), tensor(0.1333)]\n",
      "[tensor(0.1313), tensor(0.1323), tensor(0.1329)]\n",
      "[tensor(0.1312), tensor(0.1323), tensor(0.1329)]\n",
      "cladec metrics\n",
      "tensor(0.1329)\n",
      "refae metrics\n",
      "tensor(0.1333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg[\"opt\"] = ('S', 1 if dummy else 20, 0.1, 0.0001)\n",
    "cfg[\"device\"] = torch.device(\"cuda:0\")\n",
    "ae_train_loss_list = []\n",
    "ae_val_loss_list = []\n",
    "cladec_train_loss_list = []\n",
    "cladec_val_loss_list = []\n",
    "cladec_val_acc = []\n",
    "refae_val_acc = []\n",
    "\n",
    "# for i in range(4, 5):\n",
    "#     refae_path = os.path.join(model_path, \"refAE_{}.pt\".format(str(i)))\n",
    "#     cladec_path = os.path.join(model_path, \"cladecNet_{}.pt\".format(str(i)))\n",
    "#     cfg[\"layInd\"] = -1*i\n",
    "#     #get Activations\n",
    "#     actModel = getActModel(cfg, classifier)\n",
    "#     actModel.eval()\n",
    "#     trds=getActs(train_loader,actModel,cfg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(2,5)):\n",
    "    cfg[\"layInd\"] = -1*i\n",
    "    actModel = getActModel(cfg, classifier)\n",
    "    actModel.eval()\n",
    "    trds=getActs(train_dataset,actModel,cfg)\n",
    "\n",
    "    d = next(iter(trds))\n",
    "    refAE = RefAE(cfg, d[2].squeeze(0).shape).cuda()\n",
    "    refAE.load_state_dict(torch.load(os.path.join(model_path, \"refAE_\"+  str(i) + \".pt\")))\n",
    "    cladecNet = ClaDecNet(cfg, d[2].squeeze(0).shape, cfg[\"imCh\"]).cuda()\n",
    "\n",
    "    cladecNet.load_state_dict(torch.load(os.path.join(model_path, \"cladecNet_\"+  str(i) + \".pt\")))\n",
    "    refAE.eval()\n",
    "    cladecNet.eval()\n",
    "    cladec_ae = torch.nn.Sequential(actModel, cladecNet)\n",
    "    cladec_ae.eval()\n",
    "    \n",
    "    train_acts = getActs(train_dataset, actModel, cfg)\n",
    "    val_acts = getActs(val_dataset, actModel, cfg)\n",
    "    \n",
    "    #cladec_train = getReconstr(cfg, model_path, cladec=True, train=True)\n",
    "    #refae_train = getReconstr(cfg, model_path, cladec=False, train=True)\n",
    "    cladec_train_loss, refae_train_loss = getReconstrDS(train_acts, refAE, cladecNet)\n",
    "    #cladec_val= getReconstr(cfg, model_path, cladec=True, train=False)\n",
    "    #refae_val = getReconstr(cfg, model_path, cladec=False, train=False)\n",
    "    cladec_val_loss, refae_val_loss = getReconstrDS(train_acts, refAE, cladecNet)\n",
    "    \n",
    "    ae_train_loss_list.append(refae_train_loss)\n",
    "    ae_val_loss_list.append(refae_val_loss)\n",
    "    cladec_train_loss_list.append(cladec_train_loss)\n",
    "    cladec_val_loss_list.append(cladec_val_loss)\n",
    "    print(ae_train_loss_list)\n",
    "    print(ae_val_loss_list)\n",
    "    print(cladec_train_loss_list)\n",
    "    print(cladec_val_loss_list)\n",
    "\n",
    "#     if i >1:    \n",
    "#         clf_cladec, lcfg_cladec = getclassifier(cfg,  cladec_train, cladec_val, norm=None)\n",
    "#         cladec_val_acc.append(lcfg_cladec[\"testAcc\"])\n",
    "#         torch.save(clf_cladec.state_dict(),os.path.join(model_path, \"clf_cladec_\" + str(i) + \".pt\"))\n",
    "#         torch.save({\"cladec_train_loss\" : cladec_train_loss_list, \"cladec_val_loss\": cladec_val_loss_list, \"cladec_val_acc\" : cladec_val_acc}, os.path.join(model_path,\"cladec_metrics_{}.pt\".format(str(i))))\n",
    "\n",
    "    \n",
    "#     clf_ae, lcfg_ae = getclassifier(cfg,  refae_train, refae_val, norm=None)\n",
    "#     refae_val_acc.append(lcfg_ae[\"testAcc\"])\n",
    "#     torch.save(clf_ae.state_dict(),os.path.join(model_path, \"clf_ae_\" + str(i) + \".pt\"))\n",
    "#     torch.save({\"ae_train_loss\" : ae_train_loss_list, \"ae_val_loss\": ae_val_loss_list, \"ae_val_acc\" : refae_val_acc}, os.path.join(model_path, \"ae_metrics_{}.pt\".format(str(i))))\n",
    "\n",
    "print(\"cladec metrics\")\n",
    "print(cladec_train_loss)\n",
    "print(\"refae metrics\")\n",
    "print(refae_val_loss)\n",
    "\n",
    "\n",
    "#     refAE,rcfg = getRefAE(cfg,trds, train_loader) #Does not use activations themselves, only needs shape\n",
    "#     print(rcfg)\n",
    "#     torch.save(refAE.state_dict(), refae_path)\n",
    "\n",
    "#     # Train ClaDec\n",
    "#     cladecNet,ccfg=  getClaDec(cfg,classifier,None,trds)\n",
    "#     print(\"ClaDec Final loss\", ccfg)\n",
    "#     torch.save(cladecNet.state_dict(), cladec_path)\n",
    "\n",
    "    # refAE.eval()\n",
    "    # cladecNet.eval()\n",
    "    # actModel.eval()\n",
    "    \n",
    "    # train_acts = getActs(train_loader, actModel, cfg)\n",
    "    # val_acts = getActs(val_loader, actModel, cfg)\n",
    "    \n",
    "    # cladec_train = getReconstr(train_loader, torch.nn.Sequential(actModel, cladecNet),cfg, train=True)\n",
    "    # refae_train = getReconstr(train_loader, refAE,cfg, train=True)\n",
    "    # cladec_train_loss, refae_train_loss = getReconstrDS(train_acts, refAE, cladecNet)\n",
    "    # cladec_val= getReconstr(val_loader, torch.nn.Sequential(actModel, cladecNet),cfg, train=False)\n",
    "    # refae_val = getReconstr(val_loader, refAE,cfg, train=False)\n",
    "    # cladec_val_loss, refae_val_loss = getReconstrDS(train_acts, refAE, cladecNet)\n",
    "    \n",
    "    # ae_train_loss_list.append(refae_train_loss)\n",
    "    # ae_val_loss_list.append(refae_val_loss)\n",
    "    # cladec_train_loss_list.append(cladec_train_loss)\n",
    "    # cladec_val_loss_list.append(cladec_val_loss)\n",
    "\n",
    "    \n",
    "    # clf_cladec, lcfg_cladec = getclassifier(cfg,  cladec_train, cladec_val, norm=None)\n",
    "    # cladec_val_acc.append(lcfg_cladec[\"testAcc\"])\n",
    "    # torch.save(clf_cladec.state_dict(),os.path.join(model_path,\"clf_cladec_\" + str(i) + \".pt\"))\n",
    "    # clf_ae, lcfg_ae = getclassifier(cfg,  refae_train, refae_val, norm=None)\n",
    "    # refae_val_acc.append(lcfg_ae[\"testAcc\"])\n",
    "    # torch.save(clf_ae.state_dict(),os.path.join(model_path,\"clf_ae_\" + str(i) + \".pt\"))\n",
    "    # torch.save({\"cladec_train_loss\" : cladec_train_loss_list, \"cladec_val_loss\": cladec_val_loss_list, \"cladec_val_acc\" : cladec_val_acc}, os.path.join(model_path,\"cladec_metrics_{}.pt\".format(str(i))))\n",
    "    # torch.save({\"ae_train_loss\" : ae_train_loss_list, \"ae_val_loss\": ae_val_loss_list, \"ae_val_acc\" : refae_val_acc}, os.path.join(model_path,\"ae_metrics_{}.pt\".format(str(i))))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dat, img) = next(iter(cladec_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cladec_train = getReconstr(cfg, model_path, cladec=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(ae_train_loss_list)\n",
    "print(ae_val_loss_list)\n",
    "print(cladec_train_loss_list)\n",
    "print(cladec_val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949\n"
     ]
    }
   ],
   "source": [
    "print(getAcc(classifier, val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "resnet_cladec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c22cb3b195d49dab6d705a905f18142": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d2fb2afbaf64e2c84e336f6e332b754",
      "placeholder": "​",
      "style": "IPY_MODEL_dd503faeeef742eab52e87ae7b067695",
      "value": " 5120/? [00:58&lt;00:00, 88.18it/s]"
     }
    },
    "0fe934bf02294b659aab87d16f2e1bf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6370c2436f843b89240ce693b58f4dd",
       "IPY_MODEL_0c22cb3b195d49dab6d705a905f18142"
      ],
      "layout": "IPY_MODEL_ff36e47b2e1f4397b0a934d4c42f1e5a"
     }
    },
    "256b279d03d641fca6f05748506afb2a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cdbbe1ee106490084ade12486add797": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d2fb2afbaf64e2c84e336f6e332b754": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ef6458597e847d185383a25357af0b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3f9f7c15b880430ba8a5f02d7f9cda9e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44dc577cb11146fda612deb817dd9e2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d2c38bb74784716bfd1b1adc2b7cfc3",
      "max": 9912422,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6970411b2df34503ad2486672bf1cf10",
      "value": 9912422
     }
    },
    "59a0f3638bf241348a2aa6162b69db60": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6314643e016f46f58d3227069c4285f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bdc42fe22a44a96ba00666758dfeeeb",
       "IPY_MODEL_7655f32e1c6544b0bc85de272d43ba36"
      ],
      "layout": "IPY_MODEL_8b5294c3f4ad45e292f6e601150e7fb7"
     }
    },
    "6970411b2df34503ad2486672bf1cf10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "762bfde8ede74d2e9bfffdb1d0e32496": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2c30f83d4f44502a8a8ef35b01d6d07",
      "placeholder": "​",
      "style": "IPY_MODEL_f88df70769a84eaeb138fa3f93ab7bb4",
      "value": " 9913344/? [01:02&lt;00:00, 159453.99it/s]"
     }
    },
    "7655f32e1c6544b0bc85de272d43ba36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebdf382cb4fc4a89ab57e48169ca1509",
      "placeholder": "​",
      "style": "IPY_MODEL_f7a75a7b81564cc3ad8531a263dc5640",
      "value": " 29696/? [00:02&lt;00:00, 13603.69it/s]"
     }
    },
    "8b5294c3f4ad45e292f6e601150e7fb7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bdc42fe22a44a96ba00666758dfeeeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59a0f3638bf241348a2aa6162b69db60",
      "max": 28881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_99cbea3befbe4e618fe4a44efcb8b4d3",
      "value": 28881
     }
    },
    "91e126cc6cc5453c8090bbff4bef93bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99cbea3befbe4e618fe4a44efcb8b4d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9d2c38bb74784716bfd1b1adc2b7cfc3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4999b421a1047d8828791c5dacc63bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbb7bd015c114720a7c5c4aea885415e",
       "IPY_MODEL_d969f77bc761449aa684e69df606cf6d"
      ],
      "layout": "IPY_MODEL_aae552c82396469c9a23f40d46ba9e78"
     }
    },
    "aae552c82396469c9a23f40d46ba9e78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2c30f83d4f44502a8a8ef35b01d6d07": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b86844bde02044808c130622bc44dcc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6370c2436f843b89240ce693b58f4dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cdbbe1ee106490084ade12486add797",
      "max": 4542,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ef6458597e847d185383a25357af0b7",
      "value": 4542
     }
    },
    "cbb7bd015c114720a7c5c4aea885415e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f9f7c15b880430ba8a5f02d7f9cda9e",
      "max": 1648877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d45f2fcd53bb487fb359906c4e607f8d",
      "value": 1648877
     }
    },
    "d45f2fcd53bb487fb359906c4e607f8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d969f77bc761449aa684e69df606cf6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_256b279d03d641fca6f05748506afb2a",
      "placeholder": "​",
      "style": "IPY_MODEL_b86844bde02044808c130622bc44dcc7",
      "value": " 1649664/? [00:01&lt;00:00, 1266394.55it/s]"
     }
    },
    "dd503faeeef742eab52e87ae7b067695": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e061b591c44840d7bfc95f7868161920": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_44dc577cb11146fda612deb817dd9e2f",
       "IPY_MODEL_762bfde8ede74d2e9bfffdb1d0e32496"
      ],
      "layout": "IPY_MODEL_91e126cc6cc5453c8090bbff4bef93bb"
     }
    },
    "ebdf382cb4fc4a89ab57e48169ca1509": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7a75a7b81564cc3ad8531a263dc5640": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f88df70769a84eaeb138fa3f93ab7bb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff36e47b2e1f4397b0a934d4c42f1e5a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
